{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c5e733d",
   "metadata": {},
   "source": [
    "## DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22d17518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fe55352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/21 00:18:51 WARN Utils: Your hostname, jump-windows resolves to a loopback address: 127.0.1.1; using 192.168.0.5 instead (on interface wlp3s0)\n",
      "22/12/21 00:18:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/21 00:18:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/12/21 00:18:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12812d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+---------+\n",
      "|Seqno|Name        |Data     |\n",
      "+-----+------------+---------+\n",
      "|1    |john jones  |[1, 2, 3]|\n",
      "|2    |tracey smith|[4, 5, 6]|\n",
      "|3    |amy sanders |[7, 8, 9]|\n",
      "+-----+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"Seqno\",\"Name\",\"Data\"]\n",
    "data = [(\"1\", \"john jones\", [1,2,3]),\n",
    "    (\"2\", \"tracey smith\", [4,5,6]),\n",
    "    (\"3\", \"amy sanders\", [7,8,9])]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d23ea4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertCase(s: str):\n",
    "    return s.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f45c02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b7a60cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "convertUDF = udf(lambda z: convertCase(z), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09b14a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Seqno|Name        |\n",
      "+-----+------------+\n",
      "|1    |John Jones  |\n",
      "|2    |Tracey Smith|\n",
      "|3    |Amy Sanders |\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"Seqno\"), \\\n",
    "    convertUDF(col(\"Name\")).alias(\"Name\") ) \\\n",
    "   .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b39cb2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upperCase(s: str):\n",
    "    return s.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a08f887",
   "metadata": {},
   "outputs": [],
   "source": [
    "upperCaseUDF = udf(lambda z:upperCase(z), StringType())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26d7b96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+---------+-------------+\n",
      "|Seqno|Name        |Data     |Cureated Name|\n",
      "+-----+------------+---------+-------------+\n",
      "|1    |john jones  |[1, 2, 3]|JOHN JONES   |\n",
      "|2    |tracey smith|[4, 5, 6]|TRACEY SMITH |\n",
      "|3    |amy sanders |[7, 8, 9]|AMY SANDERS  |\n",
      "+-----+------------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"Cureated Name\", upperCaseUDF(col(\"Name\"))) \\\n",
    "  .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0677b122",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=StringType()) \n",
    "def splitCase(s: str):\n",
    "    return s.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c57c933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+---------+---------------+\n",
      "|Seqno|Name        |Data     |Cureated Name  |\n",
      "+-----+------------+---------+---------------+\n",
      "|1    |john jones  |[1, 2, 3]|[john, jones]  |\n",
      "|2    |tracey smith|[4, 5, 6]|[tracey, smith]|\n",
      "|3    |amy sanders |[7, 8, 9]|[amy, sanders] |\n",
      "+-----+------------+---------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"Cureated Name\", splitCase(col(\"Name\"))) \\\n",
    ".show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e820288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+---------+\n",
      "|Seqno|Name        |Data     |\n",
      "+-----+------------+---------+\n",
      "|1    |john jones  |[1, 2, 3]|\n",
      "|2    |tracey smith|[4, 5, 6]|\n",
      "|3    |null        |[7, 8, 9]|\n",
      "+-----+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"Seqno\",\"Name\",\"Data\"]\n",
    "data = [(\"1\", \"john jones\", [1,2,3]),\n",
    "    (\"2\", \"tracey smith\", [4,5,6]),\n",
    "    (\"3\", None, [7,8,9])]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41303607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/21 00:19:09 ERROR Executor: Exception in task 2.0 in stage 11.0 (TID 23)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_7999/241544004.py\", line 3, in splitCase\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:86)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/12/21 00:19:09 WARN TaskSetManager: Lost task 2.0 in stage 11.0 (TID 23) (control-plane.minikube.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_7999/241544004.py\", line 3, in splitCase\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:86)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n",
      "22/12/21 00:19:09 ERROR TaskSetManager: Task 2 in stage 11.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_7999/241544004.py\", line 3, in splitCase\nAttributeError: 'NoneType' object has no attribute 'split'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7999/1588280497.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cureated Name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplitCase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Name\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    613\u001b[0m                 )\n\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_truncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_7999/241544004.py\", line 3, in splitCase\nAttributeError: 'NoneType' object has no attribute 'split'\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"Cureated Name\", splitCase(col(\"Name\"))) \\\n",
    ".show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db22e237",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=StringType()) \n",
    "def splitCase(s: str):\n",
    "    return s.split() if s else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f920851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+---------+---------------+\n",
      "|Seqno|Name        |Data     |Cureated Name  |\n",
      "+-----+------------+---------+---------------+\n",
      "|1    |john jones  |[1, 2, 3]|[john, jones]  |\n",
      "|2    |tracey smith|[4, 5, 6]|[tracey, smith]|\n",
      "|3    |null        |[7, 8, 9]|               |\n",
      "+-----+------------+---------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"Cureated Name\", splitCase(col(\"Name\"))) \\\n",
    ".show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8ad9364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, ArrayType, FloatType\n",
    "\n",
    "@udf(returnType=ArrayType(FloatType())) \n",
    "def splitArray(s: list):\n",
    "    import numpy as np\n",
    "    if not s:\n",
    "        return ''\n",
    "    s = np.array(s)\n",
    "    s = s / np.max(s)\n",
    "    return s.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5063a22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+---------+----------------------------+\n",
      "|Seqno|Name        |Data     |Cureated Data               |\n",
      "+-----+------------+---------+----------------------------+\n",
      "|1    |john jones  |[1, 2, 3]|[0.33333334, 0.6666667, 1.0]|\n",
      "|2    |tracey smith|[4, 5, 6]|[0.6666667, 0.8333333, 1.0] |\n",
      "|3    |null        |[7, 8, 9]|[0.7777778, 0.8888889, 1.0] |\n",
      "+-----+------------+---------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"Cureated Data\", splitArray(col(\"Data\"))) \\\n",
    ".show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "059f20d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import ArrayType, FloatType, IntegerType\n",
    "\n",
    "@udf(returnType=ArrayType(FloatType()))\n",
    "def normalize(x: list):\n",
    "    x: np.array = np.array(x)\n",
    "    x = x / np.max(x)\n",
    "    return x.tolist()\n",
    "\n",
    "\n",
    "@udf(returnType=ArrayType(IntegerType()))\n",
    "def idx_to_onehot(y: int):\n",
    "    if not y or y < 0 or y >= 10:\n",
    "        return -1\n",
    "    y_true = np.zeros(10, dtype=np.int32)\n",
    "    y_true[y] = 1\n",
    "    return y_true.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64931359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|x        |y  |\n",
      "+---------+---+\n",
      "|[1, 2, 3]|3  |\n",
      "|[4, 5, 6]|4  |\n",
      "|[7, 8, 9]|1  |\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"x\",\"y\"]\n",
    "data = [([1,2,3], 3),\n",
    "    ([4,5,6], 4),\n",
    "    ([7,8,9], 1)]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e7e2d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------------------------+------------------------------+\n",
      "|x        |y  |feature                     |label                         |\n",
      "+---------+---+----------------------------+------------------------------+\n",
      "|[1, 2, 3]|3  |[0.33333334, 0.6666667, 1.0]|[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]|\n",
      "|[4, 5, 6]|4  |[0.6666667, 0.8333333, 1.0] |[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]|\n",
      "|[7, 8, 9]|1  |[0.7777778, 0.8888889, 1.0] |[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]|\n",
      "+---------+---+----------------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"feature\", normalize(f.col(\"x\"))).withColumn(\"label\", idx_to_onehot(f.col(\"y\")))\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c5d064",
   "metadata": {},
   "source": [
    "## Note:\n",
    "- PySpark UDF is a User Defined Function that is used to create a reusable function in Spark.\n",
    "- Once UDF created, that can be re-used on multiple DataFrames and SQL (after registering).\n",
    "- The default type of the udf() is StringType.\n",
    "- You need to handle nulls explicitly otherwise you will see side-effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d66c6d",
   "metadata": {},
   "source": [
    "## Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3b3d494",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da2745b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'name': 'x',\n",
       "  'type': {'type': 'array', 'elementType': 'float', 'containsNull': True},\n",
       "  'nullable': True,\n",
       "  'metadata': {}},\n",
       " {'name': 'y', 'type': 'integer', 'nullable': True, 'metadata': {}},\n",
       " {'name': 'timestamp', 'type': 'string', 'nullable': True, 'metadata': {}})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StructField(\"x\", ArrayType(FloatType())).jsonValue(),\\\n",
    "StructField(\"y\", IntegerType()).jsonValue(),\\\n",
    "StructField(\"timestamp\", StringType()).jsonValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4e741ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = {\n",
    "    \"subject\": \"entity-mnist\",\n",
    "    \"type\": \"entity\",\n",
    "    \"name\": \"mnist\",\n",
    "    \"key\": \"\",\n",
    "    \"properties\": {\n",
    "        \"type\": \"struct\",\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\": \"x\",\n",
    "                \"type\": {\"type\": \"array\", 'elementType': \"float\", \"containsNull\": True},\n",
    "                \"nullable\": True,\n",
    "                \"metadata\": {}\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"y\",\n",
    "                \"type\": \"integer\",\n",
    "                \"nullable\": True,\n",
    "                \"metadata\": {}\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"timestamp\",\n",
    "                \"type\": \"string\",\n",
    "                \"nullable\": True,\n",
    "                \"metadata\": {}\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"id\": \"63a183bff06069fd9982a5b6\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61aaa9d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('x', ArrayType(FloatType(), True), True), StructField('y', IntegerType(), True), StructField('timestamp', StringType(), True)])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StructType.fromJson(response[\"properties\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69353209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "db_uri = \"mongodb://registry_user:registry_secret@localhost:27017/registry\"\n",
    "client = pymongo.MongoClient(db_uri)\n",
    "db = client[\"registry\"]\n",
    "collection = db[\"featuregroup-mnist\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "083b45a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    \"timestamp\": {\"$gte\": \"2022-12-20T17:17:31\", \"$lte\": \"2022-12-20T17:17:41\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3e88aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(list(collection.find(query, {\"_id\": 0})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "360b5625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>feature</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>5</td>\n",
       "      <td>2022-12-20T17:17:31.467910</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-12-20T17:17:31.469386</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-12-20T17:17:31.469734</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-12-20T17:17:31.469965</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>9</td>\n",
       "      <td>2022-12-20T17:17:31.470246</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6047</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>6</td>\n",
       "      <td>2022-12-20T17:17:40.988628</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6048</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-12-20T17:17:40.988956</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6049</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-12-20T17:17:40.989274</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6050</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>7</td>\n",
       "      <td>2022-12-20T17:17:40.989665</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6051</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>9</td>\n",
       "      <td>2022-12-20T17:17:40.989989</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6052 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      x  y  \\\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  5   \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  0   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  4   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  1   \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  9   \n",
       "...                                                 ... ..   \n",
       "6047  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  6   \n",
       "6048  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  3   \n",
       "6049  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  4   \n",
       "6050  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  7   \n",
       "6051  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  9   \n",
       "\n",
       "                       timestamp  \\\n",
       "0     2022-12-20T17:17:31.467910   \n",
       "1     2022-12-20T17:17:31.469386   \n",
       "2     2022-12-20T17:17:31.469734   \n",
       "3     2022-12-20T17:17:31.469965   \n",
       "4     2022-12-20T17:17:31.470246   \n",
       "...                          ...   \n",
       "6047  2022-12-20T17:17:40.988628   \n",
       "6048  2022-12-20T17:17:40.988956   \n",
       "6049  2022-12-20T17:17:40.989274   \n",
       "6050  2022-12-20T17:17:40.989665   \n",
       "6051  2022-12-20T17:17:40.989989   \n",
       "\n",
       "                                                feature  \\\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "...                                                 ...   \n",
       "6047  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "6048  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "6049  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "6050  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "6051  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                               label  \n",
       "0     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]  \n",
       "1     [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "2     [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]  \n",
       "3     [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]  \n",
       "...                              ...  \n",
       "6047  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]  \n",
       "6048  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  \n",
       "6049  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]  \n",
       "6050  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  \n",
       "6051  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]  \n",
       "\n",
       "[6052 rows x 5 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "589ec788",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1428d7c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "fe7393ac5a12b0fa2fb8840cfa8b4da8099631a836a930f168d70e34a1f15940"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
